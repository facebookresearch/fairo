{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1785b5-f435-4017-8e00-35c8455be31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "from droidlet.lowlevel.robot_mover_utils import transform_pose\n",
    "from numba import njit\n",
    "from math import ceil, floor\n",
    "import math\n",
    "\n",
    "# Values for locobot in habitat. \n",
    "# TODO: generalize this for all robots\n",
    "fx, fy = 256, 256\n",
    "cx, cy = 256, 256\n",
    "intrinsic_mat = np.array([[  fx, 0., cx],\n",
    "                            [  0., fy, cy],\n",
    "                            [  0., 0., 1.]])\n",
    "# rotation from pyrobot to canonical coordinates (https://github.com/facebookresearch/fairo/blob/main/agents/locobot/coordinates.MD)\n",
    "rot = np.array([[0.0, 0.0, 1.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]])\n",
    "CAMERA_HEIGHT = 0.6\n",
    "trans = np.array([0, 0, CAMERA_HEIGHT])\n",
    "\n",
    " # TODO: Consolidate camera intrinsics and their associated utils across locobot and habitat.\n",
    "def compute_uvone(height, width):\n",
    "    intrinsic_mat_inv = np.linalg.inv(intrinsic_mat)\n",
    "    img_resolution = (height, width)\n",
    "    img_pixs = np.mgrid[0 : img_resolution[0] : 1, 0 : img_resolution[1] : 1]\n",
    "    img_pixs = img_pixs.reshape(2, -1)\n",
    "    img_pixs[[0, 1], :] = img_pixs[[1, 0], :]\n",
    "    uv_one = np.concatenate((img_pixs, np.ones((1, img_pixs.shape[1]))))\n",
    "    uv_one_in_cam = np.dot(intrinsic_mat_inv, uv_one)\n",
    "    return uv_one_in_cam, intrinsic_mat, rot, trans\n",
    "\n",
    "def convert_depth_to_pcd(depth, pose, uv_one_in_cam, rot, trans):\n",
    "    # point cloud in camera frame\n",
    "    depth = (depth.astype(np.float32) / 1000.0).reshape(-1)\n",
    "    pts_in_cam = np.multiply(uv_one_in_cam, depth)\n",
    "    pts_in_cam = np.concatenate((pts_in_cam, np.ones((1, pts_in_cam.shape[1]))), axis=0)\n",
    "    # point cloud in robot base frame\n",
    "    pts_in_base = pts_in_cam[:3, :].T\n",
    "    pts_in_base = np.dot(pts_in_base, rot.T)\n",
    "    pts_in_base = pts_in_base + trans.reshape(-1)\n",
    "    # point cloud in world frame (pyrobot)\n",
    "    pts_in_world = transform_pose(pts_in_base, pose)\n",
    "    return pts_in_world\n",
    "\n",
    "from scipy import ndimage \n",
    "\n",
    "def inspect(filtered, annot_img):\n",
    "    # quantize to nearest neighbor values in annot_img\n",
    "    before = np.sum(annot_img == 0)\n",
    "    after = np.sum(filtered == 0)\n",
    "    print(f'holes filed {(before-after)/before * 100} %')\n",
    "\n",
    "    # do nearest neighbor quantization\n",
    "            \n",
    "    \n",
    "    az = annot_img == 0\n",
    "    # print(f'showing \n",
    "        \n",
    "    # visualize side by side, and diff\n",
    "    diff = (filtered != annot_img) & (annot_img == 0)\n",
    "    # print(f'dtypes {annot_img.dtype, filtered.dtype}')\n",
    "    # check some values\n",
    "    \n",
    "    a = filtered[diff == True].ravel()\n",
    "    b = annot_img[diff == True].ravel()\n",
    "    \n",
    "    print(f'a {a.shape}, b {b.shape}, {a[:5], b[:5]}')\n",
    "    \n",
    "    # ndiff = a - b\n",
    "    # print(f'diff histogram')\n",
    "    # plt.hist(ndiff)\n",
    "    # plt.show()\n",
    "    \n",
    "    vis_diff = np.zeros_like(annot_img)\n",
    "    t = diff != 0\n",
    "    # print(f'diff indices {t.shape, t[:3]}')\n",
    "    \n",
    "    vis_diff[diff != 0] = 1\n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=(10,5))\n",
    "    axs[0].imshow(annot_img)\n",
    "    axs[1].imshow(filtered)\n",
    "    axs[2].imshow(vis_diff)\n",
    "    plt.show()\n",
    "    \n",
    "    # # what are the value distributions \n",
    "    # fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "    # axs[0].hist(annot_img)\n",
    "    # axs[1].hist(filtered)\n",
    "    # plt.show()\n",
    "    \n",
    "# def do_bilateral_filter(annot_img, \n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "def do_interp(annot_img, m):\n",
    "    # print(f'annot_img.shape {annot_img.shape}')\n",
    "    x = np.linspace(0,512,512).astype(np.uint16)\n",
    "    y =  np.linspace(0,512,512).astype(np.uint16)\n",
    "    # x = x[::-1]\n",
    "    # print(y[::-1])\n",
    "    \n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    # print(X, Y)\n",
    "    \n",
    "    # (n,D) \n",
    "    # (n,)\n",
    "    points = []\n",
    "    for x in range(512):\n",
    "        for y in range(512):\n",
    "            points.append((x,y))\n",
    "            \n",
    "    points = np.asarray(points)\n",
    "    # print(f'points.shape {points[:3]}')\n",
    "    \n",
    "    # print(f'annot_img.flatten().shape {annot_img.flatten().shape}')\n",
    "    ze = np.argwhere(annot_img == 0).tolist()\n",
    "    \n",
    "    # visualize zeros\n",
    "    zev = np.zeros_like(annot_img)\n",
    "    for x, y in ze:\n",
    "        zev[x][y] = 1\n",
    "        \n",
    "    plt.imshow(zev)\n",
    "    plt.title(f'visualizing holes in the image')\n",
    "    plt.show()\n",
    "    \n",
    "    # print(ze)\n",
    "    # for x,y in ze:\n",
    "    #     print(annot_img[max(x-4,0):][y])\n",
    "    interp = griddata(points, annot_img.flatten(), ze, method=m)\n",
    "    # plt.hist(interp)\n",
    "    # plt.show()\n",
    "    print(f'np.bincount {np.bincount(interp.astype(np.int32))}')\n",
    "    print(f'interp.shape {interp.shape, interp[:3]}')\n",
    "    \n",
    "    b = np.copy(annot_img)\n",
    "    \n",
    "    for i in range(len(ze)):\n",
    "        x,y = ze[i]\n",
    "        prev, now = b[x][y], interp[i]\n",
    "        if prev != now:\n",
    "            print(f'prev {b[x][y]}, now {interp[i]}')\n",
    "        b[x][y] = interp[i]\n",
    "    return b\n",
    "\n",
    "from collections import deque\n",
    "    \n",
    "# @njit\n",
    "def get_annot(\n",
    "    height, width, pts_in_cur_img, src_pts_in_cur_cam, cur_pts_in_cur_cam, src_label, valid_z, m, labels):\n",
    "    \"\"\"\n",
    "    This creates the new semantic labels of the projected points in the current image frame. Each new semantic label is the \n",
    "    semantic label corresponding to pts_in_cur_img in src_label. \n",
    "    \"\"\"\n",
    "    annot_img = np.zeros((height, width)).astype(np.float32)\n",
    "    for indx in range(len(pts_in_cur_img)):\n",
    "        r = int(indx/width)\n",
    "        c = int(indx - r*width)\n",
    "        x, y, _ = pts_in_cur_img[indx]\n",
    "        \n",
    "        # We take ceil and floor combinations to fix quantization errors\n",
    "        if floor(x) >= 0 and ceil(x) < height and floor(y) >=0 and ceil(y) < width and valid_z[indx]:\n",
    "            cur_indx = ceil(x) + ceil(y)*width\n",
    "            if src_pts_in_cur_cam[indx][2] - cur_pts_in_cur_cam[cur_indx][2] < 0.01:\n",
    "                annot_img[ceil(y)][ceil(x)] = src_label[r][c]\n",
    "                annot_img[floor(y)][floor(x)] = src_label[r][c]\n",
    "                annot_img[ceil(y)][floor(x)] = src_label[r][c]\n",
    "                annot_img[floor(y)][ceil(x)] = src_label[r][c]\n",
    "      \n",
    "    print(f'zeros {np.sum(annot_img == 0)}')\n",
    "   \n",
    "    \n",
    "    #interpolate here\n",
    "    if m == 'none':\n",
    "        return annot_img\n",
    "    \n",
    "#     from scipy import ndimage\n",
    "    dilated = ndimage.binary_dilation(annot_img).astype(annot_img.dtype)\n",
    "\n",
    "    def closest_non_zero(a, x, y):\n",
    "        \n",
    "        def get_neighbors(a, curx, cury):\n",
    "            ns = []\n",
    "            if curx > 0:\n",
    "                ns.append((curx-1, cury)) # n\n",
    "            if cury > 0:\n",
    "                ns.append((curx, cury-1)) # w\n",
    "            if cury < 511:\n",
    "                ns.append((curx, cury+1)) # e \n",
    "            if curx < 511:\n",
    "                ns.append((curx+1, cury)) # s\n",
    "#             if curx > 0 and cury > 0:\n",
    "#                 ns.append((curx-1, cury-1)) # nw\n",
    "#             if cury > 0:\n",
    "#                 ns.append((curx1, cury-1)) # ne\n",
    "#                 ns.append((curx+1, cury+1)) #se\n",
    "#                 ns.append((curx+1, cury)) # sw\n",
    "                \n",
    "#             if curx < len(a)-1 and cury\n",
    "            return ns\n",
    "\n",
    "        bfsq = deque([])\n",
    "        visited = np.zeros_like(a)\n",
    "        bfsq.append((x,y))\n",
    "        while True:\n",
    "            curx, cury = bfsq.popleft()\n",
    "            visited[curx][cury] = 1\n",
    "            if a[curx][cury] > 0:\n",
    "                return a[curx][cury]\n",
    "            ns = get_neighbors(a, curx, cury)\n",
    "            for n in ns:\n",
    "                if visited[n] == 0:\n",
    "                    bfsq.append(n)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def do_nn_fill(annot_img):\n",
    "        count = 0\n",
    "        for x in range(len(annot_img)):\n",
    "            for y in range(len(annot_img[0])):\n",
    "                if annot_img[x][y] == 0:\n",
    "                    annot_img[x][y] = closest_non_zero(annot_img, x, y)\n",
    "        print(f'{count} holes')\n",
    "        return annot_img\n",
    "    \n",
    "    # filled = do_nn_fill(annot_img)\n",
    "        \n",
    "    filtered = cv2.bilateralFilter(annot_img, 10, 50, 50)\n",
    "    # filtered = np.round(filtered)\n",
    "    \n",
    "    def quantize(filtered, og):\n",
    "        uq = np.unique(og.reshape(-1), axis=0)\n",
    "        print(f'unique values {uq}')\n",
    "        \n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return array[idx]\n",
    "        \n",
    "        qtized = 0\n",
    "        for x in range(512):\n",
    "            for y in range(512):\n",
    "                n = find_nearest(uq, filtered[x][y])\n",
    "                if n != filtered[x][y]:\n",
    "                    qtized += 1\n",
    "                filtered[x][y] = n\n",
    "        print(f'{qtized} values quantized!')\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    filtered = quantize(filtered, annot_img)\n",
    "    \n",
    "    inspect(filtered, annot_img)\n",
    "    \n",
    "    # return filtered\n",
    "\n",
    "    # print(np.unique(dilated))\n",
    "    \n",
    "#     mask = functools.reduce(np.logical_or, (annot_img==l for l in labels)) \n",
    "#     annot_img = np.where(mask, annot_img, 0.)\n",
    "    \n",
    "#     print('masked image with specific labels only')\n",
    "#     plt.imshow(annot_img)\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    # b = do_interp(annot_img, m)\n",
    "    # inspect(b, annot_img)\n",
    "    \n",
    "    # diff = interp - annot_img\n",
    "    # print(f'plotting diff between interpolated and original ...')\n",
    "    # plt.imshow(diff)\n",
    "    # plt.show()\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def visualize_mask(valid, height, width):\n",
    "    annot_img = np.zeros((height, width))\n",
    "    for indx in range(len(valid)):\n",
    "        r = int(indx/width)\n",
    "        c = int(indx - r*width)\n",
    "        annot_img[r][c] = valid[indx]\n",
    "        \n",
    "    plt.imshow(annot_img)\n",
    "    plt.title(f'visualize valid mask')\n",
    "    plt.show()\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "def visualize_pcd(xyz):\n",
    "    \n",
    "    # Pass xyz to Open3D.o3d.geometry.PointCloud and visualize\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    o3d.visualization.draw_geometries([pcd])\n",
    "\n",
    "\n",
    "def get_intersection(a, b):\n",
    "    common = []\n",
    "    for x in tqdm(a):\n",
    "        for y in b:\n",
    "            if np.allclose(x,y):\n",
    "                common.append(x)\n",
    "        if len(common) > 0:\n",
    "            print(f'{len(common)} elements in common!')\n",
    "    return common\n",
    "\n",
    "\n",
    "class LabelPropagate:\n",
    "    def __call__(self,    \n",
    "        src_img,\n",
    "        src_depth,\n",
    "        src_label,\n",
    "        src_pose,\n",
    "        cur_img,\n",
    "        cur_pose,\n",
    "        cur_depth,\n",
    "        interp,\n",
    "        labels,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        1. Gets point cloud for the source image \n",
    "        2. Transpose the point cloud based on robot location (cur_pose) \n",
    "        3. Project the point cloud back into the image frame. The corresponding semantic label for each point from the src_label becomes\n",
    "        the new semantic label in the current frame.\n",
    "        Args:\n",
    "            src_img (np.ndarray): source image to propagte from\n",
    "            src_depth (np.ndarray): source depth to propagte from\n",
    "            src_label (np.ndarray): source semantic map to propagte from\n",
    "            src_pose (np.ndarray): (x,y,theta) of the source image\n",
    "            cur_pose (np.ndarray): (x,y,theta) of current image\n",
    "            cur_depth (np.ndarray): current depth\n",
    "        \"\"\"\n",
    "\n",
    "        height, width, _ = src_img.shape\n",
    "        uv_one_in_cam, intrinsic_mat, rot, trans = compute_uvone(height, width)\n",
    "        \n",
    "        src_pts_in_world = convert_depth_to_pcd(src_depth, src_pose, uv_one_in_cam, rot, trans)\n",
    "        \n",
    "        # visualize using o3d\n",
    "        # visualize_pcd(src_pts_in_world[-100000:])\n",
    "        \n",
    "        # convert pts_in_world to current base\n",
    "        src_pts_in_cur_base = transform_pose(src_pts_in_world, (-cur_pose[0], -cur_pose[1], 0))\n",
    "        src_pts_in_cur_base = transform_pose(src_pts_in_cur_base, (0.0, 0.0, -cur_pose[2]))\n",
    "            \n",
    "        # print(pts_in_cur_base.shape, pts_in_cur_base[:3], pts_in_cur_base[:,2][:3])\n",
    "        # conver point from current base to current camera frame\n",
    "        src_pts_in_cur_cam = src_pts_in_cur_base - trans.reshape(-1)\n",
    "        src_pts_in_cur_cam = np.dot(src_pts_in_cur_cam, rot)\n",
    "        \n",
    "        # Get Valid Z\n",
    "        valid_z = src_pts_in_cur_cam[:,2] > 0\n",
    "        # print(f'valid_z {valid_z.shape, valid_z[:3]}')\n",
    "        # print((valid_z == True).sum(), (valid_z == False).sum())\n",
    "        \n",
    "        # Filter based on current depth.\n",
    "        cur_depth = (cur_depth.astype(np.float32) / 1000.0).reshape(-1)\n",
    "        cur_pts_in_cur_cam = np.multiply(uv_one_in_cam, cur_depth).T \n",
    "        # cur_pts_in_world = convert_depth_to_pcd(cur_depth, cur_pose, uv_one_in_cam, rot, trans)\n",
    "        # visualize_pcd(src_pts_in_cur_cam)\n",
    "        # visualize_pcd(cur_pts_in_cur_cam)\n",
    "        \n",
    "        def multidim_intersect(arr1, arr2):\n",
    "            arr1 = np.round(arr1, 4)\n",
    "            arr2 = np.round(arr2, 4)\n",
    "            arr1_view = arr1.T.view([('',arr1.dtype)]*arr1.shape[1]).T\n",
    "            arr2_view = arr2.T.view([('',arr2.dtype)]*arr2.shape[1]).T\n",
    "            print(arr1_view.shape, arr1_view[0], arr2_view.shape, arr2_view[:3])\n",
    "            intersected = np.intersect1d(arr1_view, arr2_view)\n",
    "            return intersected.view(arr1.dtype).reshape(-1, arr1.shape[1])\n",
    "        \n",
    "        \n",
    "        print(f'src_pts_in_cur_cam.shape, cur_pts_in_cur_cam.shape {src_pts_in_cur_cam.dtype, cur_pts_in_cur_cam.dtype}')\n",
    "            \n",
    "        # common = multidim_intersect(src_pts_in_cur_cam, cur_pts_in_cur_cam)\n",
    "        # print(f'{len(common)} elements in common!')\n",
    "        \n",
    "        # for the common points, indices won't be the same? \n",
    "        \"\"\"\n",
    "        for a point at x,y, I want to know the og depth and the current depth.\n",
    "        I can get the og depth using the index (x,y).\n",
    "        I can get the current depth using the index (x,y)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        diff =  src_pts_in_cur_cam - cur_pts_in_cur_cam\n",
    "        \n",
    "        fig, axs = plt.subplots(1,3,figsize=(10,5))\n",
    "        for i in range(3):  \n",
    "            axs[i].imshow(diff[:,i].reshape((512,512)))\n",
    "            axs[i].set_title(f'axis {i}')\n",
    "        plt.show()\n",
    "        \n",
    "        diff_d = diff[:,2] # np.linalg.norm(diff, axis=1)\n",
    "        # print(f'diff_d {diff_d.shape}')\n",
    "        # plt.hist(diff_d)\n",
    "        # plt.show()\n",
    "        \n",
    "#         print(f'pts_in_cur_cam.shape {src_pts_in_cur_cam.shape}, cur_pts_in_cam.shape {cur_pts_in_cur_cam.shape}')\n",
    "        \n",
    "#         # print(np.unique(\n",
    "#         print(f'diff.min() {diff_d.min()}, diff.max() {diff_d.max()}, diff.shape {diff_d.shape}')\n",
    "#         depth_thresh = 0.01\n",
    "#         t = np.where(diff_d < depth_thresh, True, False)\n",
    "        \n",
    "#         visualize_mask(t, height, width)\n",
    "        \n",
    "        # print(f'np.bincount {np.bincount(t)}')\n",
    "        # plt.imshow(diff_d.reshape((512,512)))\n",
    "        # plt.title(f'visualize l2 norms, diff_d')\n",
    "        # plt.show()\n",
    "        \n",
    "        # print(t.shape, t[:3])\n",
    "        # valid_z = np.logical_and(valid_z, t)    \n",
    "        # visualize_pcd(src_pts_in_cur_cam[valid_z])\n",
    "        # print(f'valid_z.shape {valid_z.shape}')\n",
    "        \n",
    "        \n",
    "        # conver pts in current camera frame into 2D pix values\n",
    "        src_pts_in_cur_img = np.matmul(intrinsic_mat, src_pts_in_cur_cam.T).T\n",
    "        # print(pts_in_cur_img.shape, pts_in_cur_img[:5])\n",
    "        \n",
    "        src_pts_in_cur_img /= src_pts_in_cur_img[:, 2].reshape([-1, 1])\n",
    "        \n",
    "        # take a mask of all valid indices\n",
    "        valid_idx = np.logical_and(\n",
    "            np.logical_and(0 <= src_pts_in_cur_img[:, 0], src_pts_in_cur_img[:, 0] < height),\n",
    "            np.logical_and(0 <= src_pts_in_cur_img[:, 1], src_pts_in_cur_img[:, 1] < width),\n",
    "        ).reshape((512,512))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fig, axs = plt.subplots(1,3,figsize=(16,8))\n",
    "        axs[0].imshow(src_img)\n",
    "        axs[0].set_title('og image')\n",
    "        axs[1].imshow(cur_img)\n",
    "        axs[1].set_title('cur image')\n",
    "        \n",
    "        vis_c = np.array(src_img, copy=True)\n",
    "        vis_c[valid_idx == False] = (0,0,0)\n",
    "        \n",
    "        axs[2].imshow(vis_c)\n",
    "        axs[2].set_title('part of og image still visible')\n",
    "        plt.show()\n",
    "        \n",
    "        # print(cur_pts_in_world[:4], pts_in_world[:4])\n",
    "        \n",
    "        # print(\n",
    "        # valid = pts_in_cur_img\n",
    "        # print(pts_in_cur_img[:5])\n",
    "        # print(pts_in_cur_img.min(), pts_in_cur_img.max())\n",
    "        \n",
    "        return get_annot(height, width, src_pts_in_cur_img, src_pts_in_cur_cam, cur_pts_in_cur_cam, src_label, valid_z, interp, labels)\n",
    "\n",
    "# LABEL_PROP_TEST_ASSETS_DIR = '/private/home/apratik/fairo/perception_handlers/label_prop_test_assets'\n",
    "# LABEL_PROP_OCCLUSION_DATA = '/checkpoint/apratik/jobs/reexplore/largerun1/baselinev3/2/instance/5/0/r1/'\n",
    "LABEL_PROP_OCCLUSION_DATA = '/home/locobotm/Downloads/r2'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import functools\n",
    "\n",
    "class LabelPropTest:\n",
    "    def __init__(self):\n",
    "        self.lp = LabelPropagate()\n",
    "        self.test_assets = LABEL_PROP_OCCLUSION_DATA\n",
    "\n",
    "    def read_test_asset_idx(self, root, img_indx):\n",
    "        src_img = cv2.imread(os.path.join(root, \"rgb/{:05d}.jpg\".format(img_indx)))\n",
    "        src_depth = np.load(os.path.join(root, \"depth/{:05d}.npy\".format(img_indx)))\n",
    "        src_label = np.load(os.path.join(root, \"seg/{:05d}.npy\".format(img_indx)))\n",
    "        # print(f'root {root}')\n",
    "        # print(f'rgb/{img_indx:05d}.jpg, depth/{img_indx:05d}.npy, seg/{img_indx:05d}.npy')\n",
    "        with open(os.path.join(root, \"data.json\"), \"r\") as f:\n",
    "            base_pose_data = json.load(f)\n",
    "        src_pose = base_pose_data[\"{}\".format(img_indx)]\n",
    "        \n",
    "        # Visualize label\n",
    "        return src_img, src_label, src_depth, src_pose, None\n",
    "    \n",
    "    def calculate_accuracy(self, act, pred, labels):\n",
    "        h, w = act.shape\n",
    "        assert act.shape == pred.shape\n",
    "        \n",
    "        mask = functools.reduce(np.logical_or, (pred==l for l in labels)) \n",
    "        masked = np.where(mask, pred, 0.)\n",
    "        # plt.imshow(masked)\n",
    "        # plt.show()\n",
    "\n",
    "        correct = np.sum(act[mask] == pred[mask])\n",
    "        total = np.sum(pred[mask] != 0)\n",
    "        \n",
    "        return correct/total\n",
    "    \n",
    "    def calculate_accuracy_v0(self, act, pred):\n",
    "        assert act.shape == pred.shape    \n",
    "        correct = np.sum(act[pred != 0] == pred[pred != 0])\n",
    "        total = np.sum(pred != 0)\n",
    "        return correct/total\n",
    "    \n",
    "    def _run_test(self, data_dir, interp):\n",
    "        \"\"\"\n",
    "        Checks that each label prop call runs in < 0.1 seconds with > 90% accuracy\n",
    "        \"\"\"\n",
    "        # for x in os.listdir(data_dir)[-1]:\n",
    "        dd = data_dir\n",
    "\n",
    "        # Each test asset folder has one source id, and one id to label propagate to\n",
    "        ps = range(1,18,6)\n",
    "        \n",
    "        # with open(os.path.join(dd, 'gtids.txt'), 'r') as f:\n",
    "        #     ids = f.readlines()\n",
    "        #     ids = [int(x.strip()) for x in ids]\n",
    "\n",
    "        src_img, src_label, src_depth, src_pose, cam_transform = self.read_test_asset_idx(dd, 0)\n",
    "        \n",
    "        accuracies, accuracies_v0 = [], []\n",
    "        \n",
    "        labels_chosen = [404,243, 133]\n",
    "        \n",
    "        for i in tqdm(range(len(ps))):\n",
    "            p = ps[i]\n",
    "            print(f'p {p}')\n",
    "            cur_img, cur_label, cur_depth, cur_pose, cam_transform = self.read_test_asset_idx(dd, p)\n",
    "\n",
    "            # src_depth = np.zeros_like(src_depth)\n",
    "            # src_depth[100,100] = 2\n",
    "\n",
    "            start = time.time()\n",
    "            prop_label = self.lp(\n",
    "                src_img, src_depth, src_label, src_pose, cur_img, cur_pose, cur_depth, interp, labels_chosen\n",
    "            )\n",
    "            \n",
    "            filtered = np.zeros_like(prop_label)\n",
    "            filtered[prop_label == 133] = 133\n",
    "            # print(f'filtered non_zero {filtered[filtered > 0].sum()}')\n",
    "        \n",
    "            time_taken = time.time() - start\n",
    "            acc = self.calculate_accuracy(cur_label, prop_label, labels_chosen)\n",
    "            acc_v0 = self.calculate_accuracy_v0(cur_label, prop_label)\n",
    "            accuracies.append(round(acc,2))\n",
    "            accuracies_v0.append(round(acc_v0,2))\n",
    "            \n",
    "            # print(f'time {time_taken}, acc {acc}')\n",
    "            \n",
    "            if True: #i == 11:\n",
    "                fig, axs = plt.subplots(1,3,figsize=(16,8))\n",
    "                axs[0].imshow(cur_label)\n",
    "                axs[0].set_title('GT Label')\n",
    "                axs[1].imshow(filtered)\n",
    "                axs[1].set_title(f'Propagated p{p}')\n",
    "                axs[2].imshow(prop_label)\n",
    "                axs[2].set_title(f'Propagated p{p}')\n",
    "                plt.savefig(\"bilateral_filtering_v1/{:04d}.jpg\".format(i))\n",
    "                plt.show()\n",
    "\n",
    "        print(f'plotting accuracies for interpolation {interp}')\n",
    "        plt.plot(range(len(accuracies)), accuracies, label='chosen labels')\n",
    "        plt.plot(range(len(accuracies)), accuracies_v0, label='all labels')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            plt.annotate(acc, (i, accuracies[i]))\n",
    "        for i, acc in enumerate(accuracies_v0):\n",
    "            plt.annotate(acc, (i, accuracies_v0[i]))\n",
    "        plt.savefig(\"accuracy.jpg\")\n",
    "        plt.show()\n",
    "            \n",
    "#         for x in range(70):\n",
    "#             # yaw = x/20\n",
    "#             delta += 0.1\n",
    "#             cur_pose[2] += 0.1\n",
    "\n",
    "#             print(f'src_pose {src_pose[2]} cur_pose {cur_pose[2]}, delta {math.degrees(cur_pose[2])}')\n",
    "#             start = time.time()\n",
    "#             prop_label = self.lp(src_img, src_depth, src_label, src_pose, cur_pose, cur_depth)\n",
    "\n",
    "#             fig, axs = plt.subplots(1,2,figsize=(10,5))\n",
    "#             # axs[0].imshow(src_img)\n",
    "#             axs[0].imshow(src_label)\n",
    "#             axs[1].set_title('Origin')\n",
    "#             # axs[2].imshow(cur_img)\n",
    "#             axs[1].imshow(prop_label)\n",
    "#             axs[1].set_title(f'delta in degrees {math.degrees(delta)}')\n",
    "#             # plt.show()\n",
    "#             plt.savefig(\"rot/{:04d}.jpg\".format(ctr))\n",
    "#             plt.show()\n",
    "#             ctr += 1\n",
    "\n",
    "#             time_taken = time.time() - start\n",
    "#             acc = self.calculate_accuracy(cur_label, prop_label)\n",
    "#             print(f'time {time_taken}, acc {acc}')\n",
    "#             # assert acc*100 > 90, f'accuracy {acc} < 90'\n",
    "        \n",
    "    def test_label_prop_nonoise(self, interp):\n",
    "        data_dir = os.path.join(self.test_assets)\n",
    "        self._run_test(data_dir, interp)\n",
    "            \n",
    "    # def test_label_prop_noise(self):\n",
    "    #     data_dir = os.path.join(self.test_assets, 'noise')\n",
    "    #     self._run_test(data_dir)\n",
    "        \n",
    "c = LabelPropTest()\n",
    "\n",
    "# x = np.zeros((512, 512)).astype(np.int32)\n",
    "# x[:100,:100] = True\n",
    "# print(np.bincount(x.flatten()))\n",
    "# plt.imshow(x)\n",
    "# plt.show()\n",
    "\n",
    "for interp in ['nearest']:\n",
    "    c.test_label_prop_nonoise(interp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:droidlet]",
   "language": "python",
   "name": "conda-env-droidlet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
